{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df = pd.read_csv('archive/abcnews-date-text.csv')\n",
    "# Download the stop words and initialize the stemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "#remove duplicates headlines\n",
    "df = df.drop_duplicates(subset='headline_text', keep='first')\n",
    "\n",
    "#function for process each line\n",
    "def process_headline(headline):\n",
    "    # Remove symbols using regex\n",
    "    headline = re.sub(r'[^\\w\\s]', '', headline)\n",
    "    # Tokenize the headline\n",
    "    words = word_tokenize(headline.lower())\n",
    "    # Remove stop words and perform stemming\n",
    "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    # Join the words back into a single string\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the processing function to each headline\n",
    "df['processed_headline'] = df['headline_text'].apply(process_headline)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Tokenize the headlines\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df['processed_headline'])\n",
    "sequences = tokenizer.texts_to_sequences(df['processed_headline'])\n",
    "\n",
    "# Pad the sequences to have equal length\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Autoencoder Training\n",
    "# Define the autoencoder architecture\n",
    "input_dim = padded_sequences.shape[1]\n",
    "latent_dim = 32\n",
    "input_layer = tf.keras.layers.Input(shape=(input_dim,))\n",
    "encoded = tf.keras.layers.Dense(latent_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(input_layer)\n",
    "decoded = tf.keras.layers.Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "# Define the autoencoder model\n",
    "autoencoder = tf.keras.models.Model(input_layer, decoded)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Train the model\n",
    "history = autoencoder.fit(padded_sequences, padded_sequences, batch_size=256,validation_split=0.1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Extract the latent representation of the headlines\n",
    "encoder = tf.keras.models.Model(input_layer, encoded)\n",
    "latent_vectors = encoder.predict(padded_sequences)\n",
    "\n",
    "# Cluster the data using K-means\n",
    "n_clusters = 10\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(latent_vectors)\n",
    "\n",
    "# Add the cluster labels to the DataFrame\n",
    "df['cluster'] = cluster_labels\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#visualize the clusters\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Reduce the dimensionality of the latent vectors for visualization\n",
    "tsvd = TruncatedSVD(n_components=2, random_state=42)\n",
    "latent_2d = tsvd.fit_transform(latent_vectors)\n",
    "\n",
    "# Add the 2D representation of the latent vectors to the DataFrame\n",
    "df['x'] = latent_2d[:, 0]\n",
    "df['y'] = latent_2d[:, 1]\n",
    "\n",
    "# Plot the clustering result\n",
    "sns.scatterplot(x='x', y='y', hue='cluster', data=df)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#visualize loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55109006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "score = silhouette_score(latent_vectors[:5000],cluster_labels[:5000])\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
